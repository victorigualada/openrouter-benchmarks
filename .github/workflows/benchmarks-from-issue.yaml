---
name: Benchmarks (from issue)

on:
  issues:
    types: [labeled]

permissions:
  contents: write
  pull-requests: write
  issues: write

concurrency:
  group: benchmarks-issue-${{ github.event.issue.number }}
  cancel-in-progress: false

env:
  PYTHON_VERSION: "3.14"
  DATASETS_REPO: victorigualada/home-assistant-datasets
  DATASETS_DIR: home-assistant-datasets
  UPSTREAM_REPO: allenporter/home-assistant-datasets

jobs:
  setup:
    if: github.event.label.name == 'run-benchmark'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      model_slug: ${{ steps.parse_model.outputs.model_slug }}
      model_id: ${{ steps.parse_model.outputs.model_id }}
      branch: ${{ steps.branch.outputs.branch }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Checkout home-assistant-datasets
        uses: actions/checkout@v4
        with:
          repository: ${{ env.DATASETS_REPO }}
          path: ${{ env.DATASETS_DIR }}
          fetch-depth: 0
          token: ${{ secrets.HOME_ASSISTANT_DATASETS_TOKEN }}

      - name: Parse model slug from issue form
        id: parse_model
        shell: bash
        run: |
          BODY="${{ github.event.issue.body }}"

          MODEL_SLUG=$(printf "%s\n" "$BODY" \
            | awk '
              /^### Model slug \(OpenRouter\)/ {found=1; next}
              /^### / && found {exit}
              found && NF {print; exit}
            ' \
            | tr -d '\r' \
            | xargs)

          if ! [[ "$MODEL_SLUG" =~ ^[^/]+/[^/]+$ ]]; then
            echo "Invalid model slug format: $MODEL_SLUG"
            exit 1
          fi

          MODEL_ID="${MODEL_SLUG#*/}"

          {
            echo "model_slug=$MODEL_SLUG"
            echo "model_id=$MODEL_ID"
          } >> "$GITHUB_OUTPUT"

      - name: Comment on issue if parsing failed
        if: ${{ failure() && steps.parse_model.outcome == 'failure' }}
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: [
                "‚ùå **Benchmark request validation failed**",
                "",
                "Please ensure **Model slug (OpenRouter)** is set to the format:",
                "",
                "`author/model`",
              ].join("\n"),
            });

      - name: Create work branch
        id: branch
        working-directory: ${{ env.DATASETS_DIR }}
        env:
          MODEL_SLUG: ${{ steps.parse_model.outputs.model_slug }}
        run: |
          set -euo pipefail

          TS="$(date -u +%Y%m%d%H%M%S)"
          BRANCH="benchmark/${MODEL_SLUG}-${TS}"
          echo "branch=${BRANCH}" >> "$GITHUB_OUTPUT"

          git checkout -b "${BRANCH}"

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          enable-cache: true
          cache-dependency-glob: |
            ${{ env.DATASETS_DIR }}/requirements_dev.txt
            ${{ env.DATASETS_DIR }}/requirements_eval.txt
          activate-environment: true

      - name: Install dependencies
        working-directory: ${{ env.DATASETS_DIR }}
        run: |
          set -euo pipefail
          
          uv pip install -r requirements_dev.txt --prerelease=allow
          uv pip install -r requirements_eval.txt --prerelease=allow

      - name: Ensure OpenRouter integration deps present
        working-directory: ${{ env.DATASETS_DIR }}
        run: |
          set -euo pipefail
          
          uv pip install "python-open-router==0.3.3"

      - name: Prepare allenporter/home-assistant-synthetic-home
        uses: actions/checkout@v4
        with:
          repository: allenporter/home-assistant-synthetic-home
          path: home-assistant-synthetic-home
          sparse-checkout: |
            custom_components

      - name: Write secrets file for !secret
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          set -euo pipefail
          
          printf 'openrouter_api_key: "%s"' "$OPENROUTER_API_KEY" > "${RUNNER_TEMP}/secrets.yaml"

      - name: Generate model YAMLs from OpenRouter
        env:
          SYNTHETIC_HOME_DIR: ${{ github.workspace }}/home-assistant-synthetic-home/
          SECRETS_FILE: ${{ runner.temp }}/secrets.yaml
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          MODEL_SLUG: ${{ steps.parse_model.outputs.model_slug }}
        working-directory: ${{ env.DATASETS_DIR }}
        run: |
          set -euo pipefail
          
          python3 "${GITHUB_WORKSPACE}/script/openrouter_fetch_model.py" \
            --models-dir "${GITHUB_WORKSPACE}/${{ env.DATASETS_DIR }}/models" \
            "${MODEL_SLUG}"

      - name: Commit and push models
        env:
          GH_TOKEN: ${{ secrets.HOME_ASSISTANT_DATASETS_TOKEN }}
          MODEL_SLUG: ${{ steps.parse_model.outputs.model_slug }}
        working-directory: ${{ env.DATASETS_DIR }}
        run: |
          set -euo pipefail
          
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          git add models/
          
          if ! git diff --cached --quiet; then
            git commit -m "Add model config: ${MODEL_SLUG}"
            git push -u origin "${{ steps.branch.outputs.branch }}"
          fi

      - name: Upload workspace artifacts
        uses: actions/upload-artifact@v4
        with:
          name: workspace-setup
          retention-days: 1
          path: |
            ${{ env.DATASETS_DIR }}
            home-assistant-synthetic-home

  collect-metrics:
    needs: setup
    runs-on: ubuntu-latest
    timeout-minutes: 180
    strategy:
      fail-fast: false
      matrix:
        include:
          # - dataset: assist
          #   collect_script: eval_collect_assist.sh
          #   metrics_script: eval_metrics_assist.sh
          - dataset: assist-mini
            collect_script: eval_collect_assist.sh
            metrics_script: eval_metrics_assist.sh
          # - dataset: questions
          #   collect_script: eval_collect_assist.sh
          #   metrics_script: eval_metrics_assist.sh
          - dataset: automations
            collect_script: eval_collect_automations.sh
            metrics_script: eval_metrics_automations.sh

    steps:
      - name: Download workspace artifacts
        uses: actions/download-artifact@v4
        with:
          name: workspace-setup

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          enable-cache: true
          cache-dependency-glob: |
            ${{ env.DATASETS_DIR }}/requirements_dev.txt
            ${{ env.DATASETS_DIR }}/requirements_eval.txt
          activate-environment: true

      - name: Install dependencies
        working-directory: ${{ env.DATASETS_DIR }}
        run: |
          set -euo pipefail
          
          uv pip install -r requirements_dev.txt --prerelease=allow
          uv pip install -r requirements_eval.txt --prerelease=allow
          uv pip install "python-open-router==0.3.3"

      - name: Expose synthetic-home at /workspaces
        run: |
          set -euo pipefail
          sudo mkdir -p /workspaces
          
          sudo ln -s "${GITHUB_WORKSPACE}/home-assistant-synthetic-home" /workspaces/home-assistant-synthetic-home

      - name: Write secrets file for !secret
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          set -euo pipefail
          
          printf 'openrouter_api_key: "%s"' "$OPENROUTER_API_KEY" > "${RUNNER_TEMP}/secrets.yaml"

      - name: Run collection (${{ matrix.dataset }})
        env:
          SYNTHETIC_HOME_DIR: ${{ github.workspace }}/home-assistant-synthetic-home/
          SECRETS_FILE: ${{ runner.temp }}/secrets.yaml
          MODEL: ${{ needs.setup.outputs.model_id }}
          DATASET_NAME: ${{ matrix.dataset }}
        working-directory: ${{ env.DATASETS_DIR }}
        run: |
          set -euo pipefail
          
          bash ./script/${{ matrix.collect_script }}

      - name: Run metrics (${{ matrix.dataset }})
        continue-on-error: true
        env:
          SYNTHETIC_HOME_DIR: ${{ github.workspace }}/home-assistant-synthetic-home/
          SECRETS_FILE: ${{ runner.temp }}/secrets.yaml
          DATASET_NAME: ${{ matrix.dataset }}
        working-directory: ${{ env.DATASETS_DIR }}
        run: |
          set -euo pipefail
          
          bash ./script/${{ matrix.metrics_script }}

      - name: Upload reports artifacts
        uses: actions/upload-artifact@v4
        with:
          name: reports-${{ matrix.dataset }}
          retention-days: 1
          path: |
            ${{ env.DATASETS_DIR }}/reports

  finalize:
    needs: [setup, collect-metrics]
    if: always() && needs.setup.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repo
        if: needs.collect-metrics.result == 'success'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Checkout home-assistant-datasets
        if: needs.collect-metrics.result == 'success'
        uses: actions/checkout@v4
        with:
          repository: ${{ env.DATASETS_REPO }}
          path: ${{ env.DATASETS_DIR }}
          ref: ${{ needs.setup.outputs.branch }}
          fetch-depth: 0
          token: ${{ secrets.HOME_ASSISTANT_DATASETS_TOKEN }}

      - name: Download all reports artifacts
        if: needs.collect-metrics.result == 'success'
        uses: actions/download-artifact@v4
        with:
          pattern: reports-*
          path: reports-artifacts
          merge-multiple: false

      - name: Merge reports
        if: needs.collect-metrics.result == 'success'
        run: |
          set -euo pipefail
          
          for dir in reports-artifacts/reports-*/; do
            rsync -av "${dir}" "${{ env.DATASETS_DIR }}/reports/"
          done

      - name: Install uv
        if: needs.collect-metrics.result == 'success'
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          enable-cache: true
          cache-dependency-glob: |
            ${{ env.DATASETS_DIR }}/requirements_dev.txt
            ${{ env.DATASETS_DIR }}/requirements_eval.txt
          activate-environment: true

      - name: Install dependencies
        if: needs.collect-metrics.result == 'success'
        working-directory: ${{ env.DATASETS_DIR }}
        run: |
          set -euo pipefail
          
          uv pip install -r requirements_dev.txt --prerelease=allow
          uv pip install -r requirements_eval.txt --prerelease=allow

      - name: Write secrets file for !secret
        if: needs.collect-metrics.result == 'success'
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          set -euo pipefail
          
          printf 'openrouter_api_key: "%s"' "$OPENROUTER_API_KEY" > "${RUNNER_TEMP}/secrets.yaml"

      - name: Build leaderboard
        if: needs.collect-metrics.result == 'success'
        id: leaderboard
        working-directory: ${{ env.DATASETS_DIR }}
        env:
          SECRETS_FILE: ${{ runner.temp }}/secrets.yaml
        run: |
          set -euo pipefail
          
          home-assistant-datasets leaderboard build

      - name: Commit and push results
        if: needs.collect-metrics.result == 'success'
        id: commit
        env:
          GH_TOKEN: ${{ secrets.HOME_ASSISTANT_DATASETS_TOKEN }}
          MODEL_SLUG: ${{ needs.setup.outputs.model_slug }}
        run: |
          set -euo pipefail
          
          BRANCH="${{ needs.setup.outputs.branch }}"
          REPO="${{ env.DATASETS_REPO }}"

          cd "${{ env.DATASETS_DIR }}"
          
          git add models/ reports/

          if git diff --cached --quiet; then
            echo "has_changes=0" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git commit -m "Benchmark: ${MODEL_SLUG} (OpenRouter)"
          git push -u origin "${BRANCH}"

          echo "has_changes=1" >> "$GITHUB_OUTPUT"

      - name: Comment PR link
        if: needs.collect-metrics.result == 'success' && steps.commit.outputs.has_changes == '1'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          MODEL_SLUG: ${{ needs.setup.outputs.model_slug }}
          MODEL_ID: ${{ needs.setup.outputs.model_id }}
        run: |
          set -euo pipefail
          ISSUE_NUMBER="${{ github.event.issue.number }}"
          REPO="${{ github.repository }}"

          ORIGIN_REPO="${{ env.DATASETS_REPO }}"
          UPSTREAM_REPO="${{ env.UPSTREAM_REPO }}"
          BRANCH="${{ needs.setup.outputs.branch }}"

          ORIGIN_OWNER="${ORIGIN_REPO%%/*}"
          BASE_BRANCH="$(gh repo view "${UPSTREAM_REPO}" --json defaultBranchRef -q .defaultBranchRef.name)"

          BRANCH_URL="https://github.com/${ORIGIN_REPO}/tree/${BRANCH}"
          CREATE_PR_URL="https://github.com/${UPSTREAM_REPO}/compare/${BASE_BRANCH}...${ORIGIN_OWNER}:${BRANCH}?expand=1"

          REPORTS_README="${{ env.DATASETS_DIR }}/reports/README.md"
          REPORT_ROW="$(grep -m 1 -F "${MODEL_ID}" "${REPORTS_README}" || true)"

          gh issue comment "$ISSUE_NUMBER" -R "$REPO" --body "$(
            cat <<EOF
          ‚úÖ **Benchmark for \`${MODEL_SLUG}\` succeeded**

          - **Branch**: [\`${BRANCH}\`](${BRANCH_URL})
          - **Create PR into** \`${UPSTREAM_REPO}\`: [Open a pull request](${CREATE_PR_URL})

          **Benchmark results**

          | Model | assist \$\${\\color{gray}\\small{\\textsf{(n=460)}}}\$\$ | assist-mini \$\${\\color{gray}\\small{\\textsf{(n=196)}}}\$\$ | questions \$\${\\color{gray}\\small{\\textsf{(n=370)}}}\$\$ | automations \$\${\\color{gray}\\small{\\textsf{(n=60)}}}\$\$ |
          | --- | --- | --- | --- | --- |
          ${REPORT_ROW}
          EOF
          )"

      - name: Notify nothing produced
        if: needs.collect-metrics.result == 'success' && steps.commit.outputs.has_changes == '0'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.issue.number,
              body: [
                "‚ö†Ô∏è **Benchmark completed, but no changes were produced**",
                "",
                "This usually means:",
                "- The benchmark results are identical to existing data, or",
                "- No new models/reports were generated.",
              ].join("\n"),
            });

      - name: Notify failure
        if: failure() || needs.collect-metrics.result == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;
            const issue_number = context.payload.issue.number;

            // Fetch jobs for this workflow run
            const jobs = await github.rest.actions.listJobsForWorkflowRun({
              owner,
              repo,
              run_id: context.runId,
            });

            const failedJob = jobs.data.jobs.find(
              job => job.conclusion === "failure"
            );

            const runUrl = `https://github.com/${owner}/${repo}/actions/runs/${context.runId}`;
            const jobUrl = failedJob?.html_url;

            const body = [
              "‚ùå **Benchmark workflow failed**",
              "",
              jobUrl
                ? `üîó **Failed job**: ${jobUrl}`
                : `üîó **Workflow run**: ${runUrl}`,
              "",
              "Please check the logs for details.",
            ].join("\n");

            await github.rest.issues.createComment({
              owner,
              repo,
              issue_number,
              body,
            });


